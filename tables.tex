
\documentclass{article} % For LaTeX2e
\usepackage{materials/iclr2020_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{materials/math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{colortbl}
\usepackage{pifont} % \ding{51} \ding{55}
\usepackage{multirow}
\usepackage{amssymb}
\usepackage{bbold}


\title{Tables in ICLR 2020: Here's What You Need to Draw Tables in \LaTeX}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Xovee Xu \thanks{ The templates are from \url{https://github.com/ICLR/Master-Template/blob/master/archive/iclr2020.zip}} 
\thanks{ \LaTeX~codes can be found at \url{https://github.com/Xovee/tables-in-iclr20}}
\thanks{ Homepage: \url{https://www.xovee.cn}
} \\ 
School of Information and Software Engineering\\
University of Electronic Science and Technology of China (UESTC)\\
Chengdu, Sichuan, China\\
\textit{xovee@live.com} \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}

\maketitle

\begin{abstract}
This guideline contains tables used in the \textbf{oral presentations} papers accepted by the International Conference on Learning Representations (ICLR). We hope this guideline could help beginners of \LaTeX~quickly known how to draw tables in academic papers. The referenced papers are from \url{https://openreview.net/group?id=ICLR.cc/2020/Conference#accept-talk}. 
\end{abstract}

\section{Introduction}\label{sec:introduction}

Before we start, here's some necessary packages used in \LaTeX~as follows:
\begin{itemize}
    \item \textit{amsmath}
    \item \textit{amsfonts}
    \item \textit{amssymb}
    \item \textit{bbold}
    \item \textit{bm}
    \item \textit{booktabs}
    \item \textit{colortbl}
    \item \textit{multirow}
    \item \textit{pifont}
    
\end{itemize}

All the tables are tested with \textbf{pdfLaTeX} compiler in \hyperlink{https://www.overleaf.com/}{Overleaf}. You can use my invitation link (\url{https://www.overleaf.com?r=969b656f&rm=d&rs=b}) to start using Overleaf. 

\section{Tables}\label{sec:tables}

One selected table in one paper in one following subsection. 

\subsection{CATER: A DIAGNOSTIC DATASET FOR
COMPOSITIONAL ACTIONS and TEMPORAL REASONING}

Table~\ref{tab:girdhar2019cater} is from Table 2(c) in \cite{girdhar2019cater}. 

\begin{table}[ht]
    \caption{Performance on the (a) 14-way atomic actions recognition, (b) 301-way compositional action recognition, and (c) 36-way localization task, for different methods.}
    \label{tab:girdhar2019cater}
    \begin{center}
    \begin{tabular}{llcc>{\columncolor[gray]{.85}[1pt]}ccc>{\columncolor[gray]{.85}[1pt]}ccc}
    \toprule
        Camera  & Model     & \#frames  & SR    & \multicolumn{3}{c}{Avg}   & \multicolumn{3}{c}{LSTM}  \\ 
                &           &           &       & Top 1 & Top 5 & $L_1$     & Top 1 & Top 5 & $L_1$     \\ \midrule
        -       & Random    & -         & -     & 2.8   & 13.8  & 3.9       & 2.8   & 13.8  & 3.9       \\ \arrayrulecolor[gray]{.85}\midrule
        Static  & Tracking  & -         & -     & 33.9  & -     & 2.4       & 33.9  & -     & 2.4       \\ \midrule
        Static  & TSN(RGB)  & 1         & -     & 7.4   & 27.0  & 3.9       & 15.3  & 50.0  & 3.0       \\ 
        Static  & TSN(RGB)  & 3         & -     & 14.1  & 38.5  & 3.2       & 25.6  & 67.2  & 2.6       \\ 
        Static  & TSN(Flow) & 1         & -     & 6.2   & 21.7  & 4.4       & 7.3   & 26.9  & 4.1       \\ 
        Static  & TSN(Flow) & 3         & -     & 9.6   & 32.2  & 3.7       & 14.0  & 43.5  & 3.2       \\ \midrule
        Static  & R3D       & 8         & 8     & 24.0  & 54.8  & 2.7       & 34.2  & 64.6  & 1.8       \\
        Static  & R3D       & 16        & 8     & 26.2  & 56.3  & 2.6       & 24.2  & 48.9  & 2.5       \\
        Static  & R3D       & 32        & 8     & 28.8  & 68.7  & 2.6       & 45.5  & 67.7  & 1.6       \\
        Static  & R3D       & 64        & 8     & 57.4  & 78.4  & 1.4       & 60.2  & 81.8  & 1.2       \\ \midrule
        Static  & R3D + NL  & 32        & 8     & 26.7  & 68.9  & 2.6       & 46.2  & 69.9  & 1.5       \\ \midrule
        Moving  & R3D       & 32        & 8     & 23.4  & 61.1  & 2.5       & 28.6  & 63.3  & 1.7       \\
        Moving  & R3D + NL  & 32        & 8     & 27.5  & 68.8  & 2.4       & 38.6  & 70.2  & 1.5       \\ \arrayrulecolor[gray]{0}
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\subsection{BackPack: Packing More into Backprop}\label{dangel2020backpack}

Table~\ref{tab:dangel2020backpack} is from Table 4 in \cite{dangel2020backpack}. 

\begin{table}[h]
    \small
    \caption{Best hyperparameter settings for all optimizers and the baselines shown in this work. In the Momentum baselines, the momentum parameter was fixed to $\rho = 0.9$. Parameters for computation of the running averages in Adam use the default values of $(\beta_1, \beta_2) = (0.9, 0.999)$. The DeepOBS baselines for all problems except CIFAR-100 use $B=128$, while the latter has $B=256$. The symbols $\ding{51}$ and $\ding{55}$ denote whether the hyperparameter setting is an interior point of the grid or not, respectively.}
    \label{tab:dangel2020backpack}
    \begin{center}
    \begin{tabular}{c|ccc|ccc|ccc|ccc}
        \multirow{2}{*}{\textbf{Curvature}}         & \multicolumn{3}{c}{mnist\_logreg}     & \multicolumn{3}{c}{fmnist\_2c2d}      & \multicolumn{3}{c}{cifar10\_3c3d}     & \multicolumn{3}{c}{cifar100\_allcnnc} \\ 
                                                    & $\alpha$      & $\lambda$ & int       & $\alpha$      & $\lambda$ & int       & $\alpha$      & $\lambda$ & int       & $\alpha$      & $\lambda$ & int       \\ \hline
        DiagGGN                                     & $10^{-3}$     & $10^{-3}$ & \ding{51} & $10^{-4}$     & $10^{-4}$ & \ding{55} & $10^{-3}$     & $10^{-2}$ & \ding{51} & -             & -         & -         \\
        DiagGGN-MC                                  & $10^{-3}$     & $10^{-3}$ & \ding{51} & $10^{-4}$     & $10^{-4}$ & \ding{55} & $10^{-3}$     & $10^{-2}$ & \ding{51} & $10^{-3}$     & $10^{-3}$ & \ding{51} \\
        KFAC                                        & $10^{-2}$     & $10^{-2}$ & \ding{51} & $10^{-3}$     & $10^{-3}$ & \ding{51} & 1             & 10        & \ding{55} & 1             & 1         & \ding{51} \\
        KFLR                                        & $10^{-2}$     & $10^{-2}$ & \ding{51} & $10^{-2}$     & $10^{-3}$ & \ding{51} & 1             & 10        & \ding{55} & -             & -         & -         \\
        KFRA                                        & $10^{-2}$     & $10^{-2}$ & \ding{51} & -             & -         & -         & -             & -         & -         & -             & -         & -         \\ \hline
        \textbf{Baseline}                           & \multicolumn{3}{c}{$\alpha$}          & \multicolumn{3}{c}{$\alpha$}          & \multicolumn{3}{c}{$\alpha$}          & \multicolumn{3}{c}{$\alpha$}          \\ \hline
        Momentum                                    & \multicolumn{3}{c}{$\approx 2.07 \cdot 10^{-2}$}  & \multicolumn{3}{c}{$\approx 2.07 \cdot 10^{-2}$}  & \multicolumn{3}{c}{$\approx 3.79 \cdot 10^{-3}$} & \multicolumn{3}{c}{$\approx 4.83 \cdot 10^{-1}$} \\
        Adam                                        & \multicolumn{3}{c}{$\approx 2.98 \cdot 10^{-4}$}  & \multicolumn{3}{c}{$\approx 1.27 \cdot 10^{-4}$}  & \multicolumn{3}{c}{$\approx 2.98 \cdot 10^{-4}$}  & \multicolumn{3}{c}{$\approx 6.95 \cdot 10^{-4}$} \\
    \hline
    \end{tabular}
    \end{center}
\end{table}

\subsection{GenDICE: Generalized Offline Estimation of Stationary Values}
\label{zhang2020gendice}

There's only one table in \cite{zhang2020gendice} and which is very simple, see Table~\ref{tab:zhang2020gendice}.

\begin{table}[ht]
    \caption{Statistics of different graphs.}
    \label{tab:zhang2020gendice}
    \begin{center}
    \begin{tabular}{lcc}
    \toprule
        \textbf{Dataset}    & \textbf{Number of Nodes}  & \textbf{Number of Edges}      \\ \hline
        BA (Small)          & 100                       & 400                           \\ 
        BA (Large)          & 500                       & 2000                          \\
        Cora                & 2708                      & 5429                          \\
        Citeseer            & 3327                      & 4731                          \\
    \bottomrule
    \end{tabular}
    \end{center}
\end{table}

\subsection{Principled Weight Initialization for Hypernetworks}
\label{chang2020principled}

Again, there's only one table in \cite{chang2020principled}, see Table~\ref{tab:chang2020principled}.

\begin{table}[ht]
    \caption{Hyperfan-in and Hyperfan-out Variance Formulae for $W_j^i = H_{jk}^i h(e[1])^k + \beta_j^i$. If $y^i=\text{ReLU}(W_j^ix^j+b^i)$, then $\mathbb{1}_{\text{ReLU}} = 1$, else if $y^i = W_j^ix^j + b^i$, then $\mathbb{1}_{\text{ReLU}} = 0$. If $b^i = G_l^ig(e[2])^l + \gamma^i$, then $\mathbb{1}_{\text{HBia}} = 1$, else if $b^i = 0$, then $\mathbb{1}_{\text{HBias}} = 0$. We initialize $h$ and $g$ with fan-in init, and $\beta_j^i, \gamma^i = 0$. For convolutional layers, we have to further divide $\text{Var}(H_{jk}^i)$ by the size of the receptive field. Uniform init: $X \sim \mathcal{U}(-\sqrt{3\text{Var}(X)}, \sqrt{3\text{Var}(X)})$. Normal init: $X \sim \mathcal{N}(0, \text{Var}(X))$.}
    \label{tab:chang2020principled}
    \begin{center}
    \begin{tabular}{llll}
        \textbf{Initialization}     & \multicolumn{1}{c}{\textbf{Variance Formula}} & \textbf{Initialization}   & \multicolumn{1}{c}{\textbf{Variance Formula}}     \\ \hline \\
        Hyperfan-in                 & Var$(H_{jk}^i) = \frac{2^{\mathbb{1}_{\text{ReLU}}}}{2^{\mathbb{1}_{\text{HBias}}}d_jd_k\text{Var}(e[1]^m)}$  & Hyperfan-out & Var$(H_{jk}^i) = \frac{2^{\mathbb{1}_{\text{ReLU}}}}{d_jd_k\text{Var}(e[1]^m)}$ \\
        Hyperfan-in                 & Var$(G_{l}^i) = \frac{2^{\mathbb{1}_{\text{ReLU}}}}{2d_l\text{Var}(e[2]^n)}$  & Hyperfan-out  & Var$(G_{l}^i) = \text{max}(\frac{2^{\mathbb{1}_{\text{ReLU}}}(1-\frac{d_j}{d_i})}{d_l\text{Var}(e[2]^n)}, 0)$ 
    \end{tabular}
    \end{center}
\end{table}

% \begin{table}[ht]
%     \caption{Caption}
%     \label{tab:my_label}
%     \begin{center}
%     \begin{tabular}{cc}
%          &  \\
%          & 
%     \end{tabular}
%     \end{center}
% \end{table}

% \subsection{Tables}

% All tables must be centered, neat, clean and legible. Do not use hand-drawn
% tables. The table number and title always appear before the table. See
% Table~\ref{sample-table}.

% Place one line space before the table title, one line space after the table
% title, and one line space after the table. The table title must be lower case
% (except for first word and proper nouns); tables are numbered consecutively.

% \begin{table}[t]
% \caption{Sample table title}
% \label{sample-table}
% \begin{center}
% \begin{tabular}{ll}
% \multicolumn{1}{c}{\bf PART}  &\multicolumn{1}{c}{\bf DESCRIPTION}
% \\ \hline \\
% Dendrite         &Input terminal \\
% Axon             &Output terminal \\
% Soma             &Cell body (contains cell nucleus) \\
% \end{tabular}
% \end{center}
% \end{table}

\subsubsection*{Acknowledgments}
Use unnumbered third level headings for the acknowledgments. All
acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{xovee}
\bibliographystyle{materials/iclr2020_conference}


\end{document}
